{
  "title": "Machine Learning Simulator",
  "description": "Visualize and understand how machine learning algorithms work by adjusting parameters and watching the training process in real-time.",
  "algorithm_selector_label": "Select Algorithm",
  "run_simulation_button": "Run Simulation",
  "training_button_progress": "Training ({{current}}/{{total}})",
  "algorithm_linear_regression": "Linear Regression",
  "algorithm_logistic_regression": "Logistic Regression",
  "algorithm_kmeans_clustering": "K-Means Clustering",
  "algorithm_decision_tree": "Decision Tree",
  "parameters_title": "Parameters",
  "parameter_learning_rate": "Learning Rate",
  "parameter_iterations": "Iterations",
  "parameter_regularization": "Regularization",
  "parameter_noise_level": "Noise Level",
  "parameter_desc_learning_rate_default": "Controls how quickly the model adapts to the problem",
  "parameter_desc_learning_rate_linear_regression": "Step size in gradient descent",
  "parameter_desc_learning_rate_logistic_regression": "Step size in gradient descent",
  "parameter_desc_learning_rate_kmeans_clustering": "Controls centroid movement speed",
  "parameter_desc_learning_rate_decision_tree": "Learning rate for tree building",
  "parameter_desc_iterations_default": "Number of training steps",
  "parameter_desc_iterations_linear_regression": "Number of gradient descent steps",
  "parameter_desc_iterations_logistic_regression": "Number of gradient descent iterations",
  "parameter_desc_iterations_kmeans_clustering": "Maximum number of clustering iterations",
  "parameter_desc_iterations_decision_tree": "Maximum number of splits to consider",
  "parameter_desc_regularization_default": "Prevents overfitting by penalizing large coefficients",
  "parameter_desc_regularization_linear_regression": "L2 regularization strength",
  "parameter_desc_regularization_logistic_regression": "L2 regularization strength",
  "parameter_desc_regularization_kmeans_clustering": "Cluster compactness parameter",
  "parameter_desc_regularization_decision_tree": "Tree pruning strength",
  "parameter_desc_noise_level_default": "Amount of randomness in generated data",
  "data_generation_title": "Data Generation",
  "data_description_linear_regression": "Generate points for linear relationship with noise",
  "data_description_logistic_regression": "Generate binary classification data points",
  "data_description_kmeans_clustering": "Generate clustered data points",
  "data_description_decision_tree": "Generate hierarchical data for decision boundaries",
  "data_description_default": "Generate random data for selected algorithm",
  "data_points_label": "Data Points",
  "data_current_count": "Current: {{count}} data points",
  "generate_new_data_button": "Generate New Data",
  "metrics_panel_title": "Performance Metrics",
  "metric_name_mse": "Mean Squared Error",
  "metric_name_r2": "R² Score",
  "metric_name_slope": "Slope",
  "metric_name_intercept": "Intercept",
  "metric_name_accuracy": "Accuracy",
  "metric_name_precision": "Precision",
  "metric_name_recall": "Recall",
  "metric_name_f1_score": "F1 Score",
  "metric_name_inertia": "Inertia",
  "metric_name_silhouette_score": "Silhouette Score",
  "metric_name_iterations": "Iterations",
  "metric_name_clusters": "Clusters",
  "metric_name_depth": "Depth",
  "metric_name_nodes": "Nodes",
  "metric_name_gini_impurity": "Gini Impurity",
  "interpretation_title": "Interpretation",
  "interpretation_linear_regression": "The model explains {{r2_percentage}}% of the variance in the data. The mean squared error is {{mse}}, indicating the average squared difference between the predicted and actual values.",
  "interpretation_logistic_regression": "The model correctly classifies {{accuracy_percentage}}% of the data points. The precision of {{precision_percentage}}% indicates the proportion of positive identifications that were actually correct, while recall of {{recall_percentage}}% shows the proportion of actual positives that were correctly identified.",
  "interpretation_kmeans_clustering": "The algorithm converged after {{iterations}} iterations. The inertia value of {{inertia}} represents the sum of squared distances of samples to their closest cluster center. The silhouette score of {{silhouette}} indicates how well-separated the clusters are (values close to 1 are better).",
  "interpretation_decision_tree": "The decision tree has a depth of {{depth}} with {{nodes}} nodes. It achieved an accuracy of {{accuracy_percentage}}% on the data. The average Gini impurity of {{gini}} indicates the quality of the splits (lower values mean better splits).",
  "interpretation_default": "No interpretation available for the current algorithm.",
  "footer_copyright": "© {{year}} ML Simulator. All rights reserved.",
  "footer_made_with": "Made with",
  "footer_for_enthusiasts": "for ML enthusiasts",
  "footer_privacy": "Privacy",
  "footer_terms": "Terms",
  "footer_contact": "Contact",
  "navbar_home": "Home",
  "navbar_algorithms": "Algorithms",
  "navbar_about": "About",
  "navbar_github": "GitHub",
  "theme_toggle_label_dark": "Switch to dark mode",
  "theme_toggle_label_light": "Switch to light mode",
  "visualization_title": "Visualization",
  "visualization_progress": "Progress:",
  "visualization_description_linear_regression": "The visualization shows data points and the best-fit line. As the model trains, the line adjusts to minimize the distance between itself and the data points.",
  "visualization_description_logistic_regression": "The visualization shows binary classification of data points and the decision boundary. During training, the boundary adjusts to better separate the classes.",
  "visualization_description_kmeans_clustering": "The visualization shows data points colored by their assigned cluster. Cluster centroids are shown as larger points. Watch as the centroids move to the center of each cluster.",
  "visualization_description_decision_tree": "The visualization shows the decision boundaries created by the decision tree. As the tree grows, the boundaries become more complex to fit the data.",
  "visualization_description_default": "Adjust parameters and click \"Run Simulation\" to see the algorithm in action.",
  "viz_axis_x_label": "X",
  "viz_axis_y_label": "Y",
  "viz_linear_regression_equation": "y = {{slope}}x + {{intercept}}",
  "viz_logistic_regression_equation": "Decision Boundary: {{w0}}x + {{w1}}y + {{bias}} = 0",
  "viz_kmeans_centroid_label": "C{{index}}",
  "supervised_learning_title": "Supervised Learning",
  "supervised_learning_description": "Supervised learning algorithms are trained on labeled data, meaning the training data includes both the input features and the desired output (labels). The goal is to learn a mapping from inputs to outputs so the model can predict the output for new, unseen data. Common tasks include classification (predicting a category) and regression (predicting a continuous value).",
  "unsupervised_learning_title": "Unsupervised Learning",
  "unsupervised_learning_description": "Unsupervised learning algorithms are trained on unlabeled data. The goal is to find hidden patterns, structures, or relationships within the data without specific guidance. Common tasks include clustering (grouping similar data points), dimensionality reduction (reducing the number of features), and anomaly detection (finding unusual data points).",
  "learning_type_supervised": "Supervised",
  "learning_type_unsupervised": "Unsupervised",
  "examples_title": "Examples",
  "deep_learning_intro": "Beyond these fundamental algorithms, a significant area of machine learning is Deep Learning, which utilizes artificial neural networks with multiple layers to model complex patterns in data.",
  "algorithms_intro_part1": "Machine learning is a branch of artificial intelligence that enables computer systems to learn from data without being explicitly programmed. There are various algorithms for different tasks.",
  "algorithms_intro_part2": "Here are detailed explanations of some fundamental machine learning algorithms used in this simulator:",
  "algorithm_linear_regression_description": "Linear Regression is a supervised learning algorithm used for predicting continuous output values. It establishes a linear relationship between the dependent variable and one or more independent variables.",
  "concepts_title": "Concepts",
  "algorithm_concept_dependent_variable": "Dependent Variable (Target): The value to be predicted.",
  "algorithm_concept_independent_variable": "Independent Variable (Feature): The input value(s) used to make the prediction.",
  "algorithm_concept_model_parameters": "Model Parameters (Weights/Coefficients and Bias/Intercept): Values learned by the model that define the regression line.",
  "algorithm_concept_cost_function": "Cost Function: A function that measures how well the model is performing (e.g., Mean Squared Error - MSE).",
  "algorithm_concept_gradient_descent": "Gradient Descent: An optimization method to update model parameters by minimizing the cost function.",
  "principle_title": "Working Principle",
  "algorithm_linear_regression_principle": "Linear regression attempts to express the relationship between data points with an equation like \\( y = mx + b \\) (univariate) or \\( y = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n \\) (multivariate). The algorithm learns the values of 'm' (slope/weights) and 'b' (intercept/bias) that define the line (or hyperplane) closest to the data points, using optimization techniques like Gradient Descent. This learning process focuses on minimizing the difference (error) between predicted and actual values.",
  "usecases_title": "Use Cases",
  "algorithm_usecase_house_price": "House price prediction",
  "algorithm_usecase_stock_price": "Stock price prediction",
  "algorithm_usecase_sales_prediction": "Sales prediction",
  "algorithm_usecase_performance_analysis": "Performance analysis",
  "algorithm_linear_regression_example": "**House Price Prediction:** A common use case for Linear Regression is predicting house prices. As you can see in the visualization below, linear regression is used to model the relationship between house size (independent variable) and house price (dependent variable). The algorithm learns the best-fit line (best-fit line on the scatter plot) for the existing data points to predict the price based on the house size. This line represents a linear relationship showing how price generally increases as house size increases.",
  "algorithm_linear_regression_pro": "**Advantages:** Simple, fast, interpretable.",
  "algorithm_linear_regression_con": "**Disadvantages:** Requires linear relationships, sensitive to outliers, may require feature scaling for multivariate data.",
  "algorithm_logistic_regression_description": "Logistic Regression is a supervised learning algorithm used for predicting categorical outputs (usually two classes). It uses the sigmoid function to estimate the probability of an event occurring.",
  "algorithm_concept_sigmoid_function": "Sigmoid Function: A function that squashes any real number into a value between 0 and 1.",
  "algorithm_concept_decision_boundary": "Decision Boundary: A line or hyperplane that separates the two classes.",
  "algorithm_concept_probability_estimation": "Probability Estimation: Logistic regression estimates the probability of an instance belonging to a specific class, rather than predicting the class label directly.",
  "algorithm_concept_cross_entropy_loss": "Cross-Entropy Loss: A cost function used in classification models.",
  "algorithm_logistic_regression_principle": "Logistic regression obtains a probability by transforming the output of a linear equation with the sigmoid function. The probability of an example belonging to a particular class is calculated. The model uses Gradient Descent to minimize a cost function like Cross-Entropy Loss. After training, classification is typically done using a threshold value, usually 0.5.",
  "algorithm_usecase_spam_detection": "Email spam detection",
  "algorithm_usecase_disease_diagnosis": "Disease diagnosis (present/absent)",
  "algorithm_usecase_credit_risk": "Credit risk analysis",
  "algorithm_usecase_customer_churn": "Customer churn prediction",
  "algorithm_logistic_regression_pro": "**Advantages:** Simple, fast, provides probability-based output, interpretable.",
  "algorithm_logistic_regression_con": "**Disadvantages:** Primarily designed for binary classification (extensions exist for multi-class), performs better on linearly separable data, may struggle to model complex relationships.",
  "algorithm_kmeans_clustering_description": "K-Means Clustering is an unsupervised learning algorithm used to group (cluster) data points based on their similarity. It creates a specified number of clusters (k) and assigns each point to the nearest cluster center.",
  "algorithm_concept_k_value": "K: The number of clusters to be created (must be determined before running the algorithm).",
  "algorithm_concept_centroid": "Centroid: A point representing the center of each cluster.",
  "algorithm_concept_assignment_step": "Assignment Step: The step where each data point is assigned to the cluster of the nearest centroid.",
  "algorithm_concept_update_step": "Update Step: The step where each centroid is recalculated as the mean (center) of all data points assigned to its cluster.",
  "algorithm_concept_inertia": "Inertia: The sum of squared distances of samples to their closest cluster center (measures cluster compactness).",
  "algorithm_kmeans_clustering_principle_intro": "The algorithm follows these steps:",
  "algorithm_kmeans_clustering_principle_step1": "Initialization: 'k' centroids are randomly selected or determined.",
  "algorithm_kmeans_clustering_principle_step2": "Assignment: Each data point is assigned to the cluster whose centroid is closest to it. Euclidean distance is commonly used as the distance metric.",
  "algorithm_kmeans_clustering_principle_step3": "Update: The centroid of each cluster is recalculated as the mean of all data points assigned to that cluster.",
  "algorithm_kmeans_clustering_principle_step4": "Iteration: The Assignment and Update steps are repeated until the centroids no longer change significantly or a predetermined maximum number of iterations is reached.",
  "algorithm_kmeans_clustering_principle_outro": "The goal of the algorithm is to minimize intra-cluster distances (Inertia).",
  "algorithm_usecase_customer_segmentation": "Customer segmentation",
  "algorithm_usecase_image_compression": "Image compression",
  "algorithm_usecase_anomaly_detection": "Anomaly detection",
  "algorithm_usecase_document_analysis": "Document analysis and clustering",
  "algorithm_kmeans_clustering_pro": "**Advantages:** Simple and easy to understand, effective on large datasets, a good starting point for unsupervised learning.",
  "algorithm_kmeans_clustering_con": "**Disadvantages:** The value of 'k' must be determined beforehand, sensitive to the initial choice of centroids (different starts can yield different results), can get stuck in local optima instead of global optima, struggles to separate clusters of global shape, sensitive to outliers.",
  "algorithm_decision_tree_description": "Decision Tree is a versatile supervised learning algorithm that makes decisions by splitting the data based on features. It can be used for both classification and regression tasks and builds a tree-like structure.",
  "algorithm_concept_root_node": "Root Node: The starting point of the tree, representing the entire dataset.",
  "algorithm_concept_decision_node": "Decision Node: A node that tests on a feature to split the data into subsets.",
  "algorithm_concept_leaf_node": "Leaf Node: A terminal node that contains a classification label or a regression value.",
  "algorithm_concept_splitting": "Splitting: The process of dividing a dataset into subsets based on a feature and a threshold value.",
  "algorithm_concept_pruning": "Pruning: The process of removing branches or nodes from the tree to prevent overfitting.",
  "algorithm_concept_information_gain": "Information Gain: A criterion used to select the best split (measures the reduction in entropy).",
  "algorithm_concept_gini_impurity": "Gini Impurity: A criterion used to select the best split (measures the impurity of a set of samples).",
  "algorithm_decision_tree_principle": "The decision tree algorithm recursively splits the dataset by finding the best split. The best split is determined by the feature and threshold value that typically maximizes information gain or minimizes Gini impurity. This splitting process continues until each subset is pure (belongs to a single class) or another stopping criterion (e.g., maximum depth, minimum samples per leaf) is met. The result is a decision tree structure.",
  "algorithm_usecase_customer_behavior": "Customer behavior analysis",
  "algorithm_usecase_medical_diagnosis": "Medical diagnosis",
  "algorithm_usecase_fraud_detection": "Fraud detection",
  "algorithm_usecase_recommendation_system": "Recommendation systems",
  "algorithm_decision_tree_pro": "**Advantages:** Easy to understand and interpret, requires little data preprocessing, can model non-linear relationships.",
  "algorithm_decision_tree_con": "**Disadvantages:** Prone to overfitting, can be sensitive to small variations in data, finding the optimal tree is an NP-hard problem (heuristic methods are typically used), performance can degrade on imbalanced datasets.",
  "running_simulation_button": "Running Simulation..."
} 